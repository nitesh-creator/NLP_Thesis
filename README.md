# NLP_Thesis
Word Embedding or Word Vector is a numeric vector input that represents a word in a lower-dimensional space. It allows words with similar meaning to have a similar representation. They can also approximate meaning.There are several methods for word embedding. However, most word embeddings fail to detect asymmetric word relations. So, in this study, we will examine several methods of word embedding such as TF-IDF, BERT, DOC2VEC,Glove, and GloVe Hyperbolic Word Embedding and their performance in textual data classification with the Linear SVC model, Random Forest Classifier, and MLP classifier.
#Problem Statement
We are given an eCommerce dataset with product categories and product descriptions. We need to perform multi classification with different approach.
The Dataset has the following Feature:
Data Set Characteristics: Multivariate
Number of Instances: 50425
Number of classes: 4
Associated Tasks: Classification

